[
  {
    "path": "posts/2021-09-12-trying-out-infer/",
    "title": "Trying out infer",
    "description": "This is ought to be a short blog post, firstly to get to grips with Distill, secondly to try out infer.",
    "author": [
      {
        "name": "theCed7",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-12",
    "categories": [],
    "contents": "\n\nContents\nGetting started\nStarting off with infer\nspecify: What do we want to know?\nhypothesize: declaring the null hypothesis\ngenerate: generating the null distribution\ncalculate: calculating summary statistics\nShort interlude: Visualization\nBack to infer: t-Test\n\n\nAnother one…\nCharts.\nInfer\n\n\nGetting started\nFirst things first: Let’s load infer and the rest of the tidyverse:\n\n\nlibrary(infer)\nlibrary(tidyverse)\n\n\n\nInfer is a tidyverse package which aims to aid statistical testing. As such, we also need some data to conduct statistical testing on. As it is 2021, we might as well use something coronavirus-related. This download the vast Our World in Data source file and reads it into R. I will also focus on the UK, since it’s one of the countries with the most cohesive data available.\n\n\n#download.file(\"https://github.com/owid/covid-19-data/raw/master/public/data/owid-covid-data.csv\", \"owid-covid-data.csv\")\n\ndata <- read_csv(\"owid-covid-data.csv\", \n                 col_types = cols(date = col_date(format = \"%Y-%m-%d\")))\n\ndata <- data %>%\n  filter(iso_code %in% c(\"GBR\", \"USA\", \"DEU\")) %>%\n  filter(date > \"2021-01-01\")\n\nglimpse(data)\n\n\nRows: 756\nColumns: 62\n$ iso_code                              <chr> \"DEU\", \"DEU\", \"DEU\", \"…\n$ continent                             <chr> \"Europe\", \"Europe\", \"E…\n$ location                              <chr> \"Germany\", \"Germany\", …\n$ date                                  <date> 2021-01-02, 2021-01-0…\n$ total_cases                           <dbl> 1773540, 1783896, 1796…\n$ new_cases                             <dbl> 10903, 10356, 12320, 1…\n$ new_cases_smoothed                    <dbl> 18185.71, 17893.86, 17…\n$ total_deaths                          <dbl> 34480, 34791, 35748, 3…\n$ new_deaths                            <dbl> 335, 311, 957, 1009, 1…\n$ new_deaths_smoothed                   <dbl> 647.714, 642.000, 657.…\n$ total_cases_per_million               <dbl> 21138.62, 21262.05, 21…\n$ new_cases_per_million                 <dbl> 129.952, 123.432, 146.…\n$ new_cases_smoothed_per_million        <dbl> 216.753, 213.275, 210.…\n$ total_deaths_per_million              <dbl> 410.963, 414.670, 426.…\n$ new_deaths_per_million                <dbl> 3.993, 3.707, 11.406, …\n$ new_deaths_smoothed_per_million       <dbl> 7.720, 7.652, 7.838, 7…\n$ reproduction_rate                     <dbl> 0.94, 0.96, 0.98, 1.00…\n$ icu_patients                          <dbl> 5703, 5745, 5723, 5650…\n$ icu_patients_per_million              <dbl> 67.973, 68.474, 68.212…\n$ hosp_patients                         <dbl> NA, NA, NA, NA, NA, NA…\n$ hosp_patients_per_million             <dbl> NA, NA, NA, NA, NA, NA…\n$ weekly_icu_admissions                 <dbl> NA, NA, NA, NA, NA, NA…\n$ weekly_icu_admissions_per_million     <dbl> NA, NA, NA, NA, NA, NA…\n$ weekly_hosp_admissions                <dbl> NA, 7777.015, NA, NA, …\n$ weekly_hosp_admissions_per_million    <dbl> NA, 92.693, NA, NA, NA…\n$ new_tests                             <dbl> NA, NA, NA, NA, NA, NA…\n$ total_tests                           <dbl> NA, 36016829, NA, NA, …\n$ total_tests_per_thousand              <dbl> NA, 429.280, NA, NA, N…\n$ new_tests_per_thousand                <dbl> NA, NA, NA, NA, NA, NA…\n$ new_tests_smoothed                    <dbl> 125811, 120818, 128689…\n$ new_tests_smoothed_per_thousand       <dbl> 1.500, 1.440, 1.534, 1…\n$ positive_rate                         <dbl> NA, 0.154, NA, NA, NA,…\n$ tests_per_case                        <dbl> NA, 6.5, NA, NA, NA, N…\n$ tests_units                           <chr> \"tests performed\", \"te…\n$ total_vaccinations                    <dbl> 280781, 303333, 349385…\n$ people_vaccinated                     <dbl> 279853, 302401, 348402…\n$ people_fully_vaccinated               <dbl> 928, 932, 983, 1070, 1…\n$ total_boosters                        <dbl> NA, NA, NA, NA, NA, NA…\n$ new_vaccinations                      <dbl> 48088, 22552, 46052, 5…\n$ new_vaccinations_smoothed             <dbl> 42738, 39854, 43848, 4…\n$ total_vaccinations_per_hundred        <dbl> 0.33, 0.36, 0.42, 0.48…\n$ people_vaccinated_per_hundred         <dbl> 0.33, 0.36, 0.42, 0.48…\n$ people_fully_vaccinated_per_hundred   <dbl> 0.00, 0.00, 0.00, 0.00…\n$ total_boosters_per_hundred            <dbl> NA, NA, NA, NA, NA, NA…\n$ new_vaccinations_smoothed_per_million <dbl> 509, 475, 523, 526, 52…\n$ stringency_index                      <dbl> 82.41, 82.41, 82.41, 8…\n$ population                            <dbl> 83900471, 83900471, 83…\n$ population_density                    <dbl> 237.016, 237.016, 237.…\n$ median_age                            <dbl> 46.6, 46.6, 46.6, 46.6…\n$ aged_65_older                         <dbl> 21.453, 21.453, 21.453…\n$ aged_70_older                         <dbl> 15.957, 15.957, 15.957…\n$ gdp_per_capita                        <dbl> 45229.25, 45229.25, 45…\n$ extreme_poverty                       <dbl> NA, NA, NA, NA, NA, NA…\n$ cardiovasc_death_rate                 <dbl> 156.139, 156.139, 156.…\n$ diabetes_prevalence                   <dbl> 8.31, 8.31, 8.31, 8.31…\n$ female_smokers                        <dbl> 28.2, 28.2, 28.2, 28.2…\n$ male_smokers                          <dbl> 33.1, 33.1, 33.1, 33.1…\n$ handwashing_facilities                <dbl> NA, NA, NA, NA, NA, NA…\n$ hospital_beds_per_thousand            <dbl> 8, 8, 8, 8, 8, 8, 8, 8…\n$ life_expectancy                       <dbl> 81.33, 81.33, 81.33, 8…\n$ human_development_index               <dbl> 0.947, 0.947, 0.947, 0…\n$ excess_mortality                      <dbl> NA, 38.85, NA, NA, NA,…\n\nStarting off with infer\nThe idea of infer is to simplfy statistical testing into four easy steps:\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nWe now go through each of these steps one by one.\nspecify: What do we want to know?\nFirst of all, we have to clarify what issue we want to test. What variable of our data frame are we interested in? In this blog post we will ask ourselves, whether there was a significant difference between the reported incidence of SARS-Cov-2 cases between Great Britain and the USA. Has one country had on average less new cases (per million) than the other?`\nSo we filter the data to compare Great Britain and the USA. Then we specify what variables we are interested in. We also declare, what we believe is cause and effect, e. g. the different countries should explain (explanatory), why there is a difference in new cases (response).\n\n\ndata %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  specify(explanatory = iso_code, response = new_cases_per_million )\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\n# A tibble: 504 × 2\n   new_cases_per_million iso_code\n                   <dbl> <fct>   \n 1                  848. GBR     \n 2                  809. GBR     \n 3                  864. GBR     \n 4                  896. GBR     \n 5                  917. GBR     \n 6                  774. GBR     \n 7                 1000. GBR     \n 8                  881. GBR     \n 9                  807. GBR     \n10                  678. GBR     \n# … with 494 more rows\n\nhypothesize: declaring the null hypothesis\nSecondly, we declare the null hypothesis. Statistically speaking, the null hypothesis assumes there truly is no difference (or no correlation). Applied to our example our null hypothesis would be: There is no significant difference in new SARS-Cov2 cases per million people between the US and the UK.\nOn a side note: We also introduce a new, shorter way of using specify. Keep an eye on which order you place the arguments (i. e. column names) in, because that determines which is used as the dependent or independent variable.\nDo not pay a lot of attention on the output our code is generating at this moment. Hopefully, this all comes together shortly. As long as there aren’t any errors and you roughly understand the statistical lingo, we should be fine.\n\n\ndata %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  specify(new_cases_per_million ~ iso_code) %>% \n  hypothesize(null = \"independence\")\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\nNull Hypothesis: independence\n# A tibble: 504 × 2\n   new_cases_per_million iso_code\n                   <dbl> <fct>   \n 1                  848. GBR     \n 2                  809. GBR     \n 3                  864. GBR     \n 4                  896. GBR     \n 5                  917. GBR     \n 6                  774. GBR     \n 7                 1000. GBR     \n 8                  881. GBR     \n 9                  807. GBR     \n10                  678. GBR     \n# … with 494 more rows\n\ngenerate: generating the null distribution\nNot quite sure what happens here.\n\n\ndata %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  specify(iso_code ~ new_cases_per_million) %>% \n  hypothesize(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\")\n\n\nResponse: iso_code (factor)\nExplanatory: new_cases_per_million (numeric)\nNull Hypothesis: independence\n# A tibble: 504,000 × 3\n# Groups:   replicate [1,000]\n   iso_code new_cases_per_million replicate\n   <fct>                    <dbl>     <int>\n 1 USA                       848.         1\n 2 GBR                       809.         1\n 3 GBR                       864.         1\n 4 USA                       896.         1\n 5 USA                       917.         1\n 6 USA                       774.         1\n 7 GBR                      1000.         1\n 8 USA                       881.         1\n 9 USA                       807.         1\n10 GBR                       678.         1\n# … with 503,990 more rows\n\ncalculate: calculating summary statistics\nNow that we have some understanding of how to use the infer functions, we try to get back to our original question.\nShort interlude: Visualization\nBefore we move on with facts, figures and infer, let’s try to get a rough picture of our question with some basic data visualization.\n\n\ndata %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  ggplot(aes(x = iso_code, y = new_cases_per_million)) +\n  geom_jitter(alpha = 0.25) + \n  geom_boxplot(alpha = 0.75) +\n  theme_minimal()\n\n\n\n\nHere we already see that both medians lie quite close together. So, aren’t these countries so different after all?\nBack to infer: t-Test\nSo, let’s calculate how big the difference between both medians actually is:\n\n\nobserved_diff_in_medians <- data %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  specify(new_cases_per_million ~ iso_code) %>% \n  calculate(stat = \"diff in medians\", order = c(\"USA\", \"GBR\"))\n\nobserved_diff_in_medians\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  3.58\n\nSo, stat = diff in medians does what it says: It calculates the difference in medians. The order specifies the order of the subtraction (i. e. changes the sign.)\nJust for illustration purposes, we can also try to calculate the difference in medians without infer:\n\n\ndata %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  group_by(iso_code) %>% \n  summarise(\n    median = median(new_cases_per_million),\n  ) %>% \n  summarise(\n    iso_code = iso_code,\n    median = median,\n    diff = diff(median),\n  )\n\n\n# A tibble: 2 × 3\n  iso_code median  diff\n  <chr>     <dbl> <dbl>\n1 GBR        173.  3.58\n2 USA        177.  3.58\n\nBut back to infer: We know now the difference between medians of new_cases_per_million in both countries equals 3.576. Is this enough do discard our null hypothesis?\nLet’s use infer to generate some data under the assumption, that in fact, the null hypothesis is true and there genuinely is no significant difference in new cases between both countries. Then we calculate the diff in medians for each replicate of the generated data.\n\n\nnull_dist_sample <- data %>% \n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \n  specify(new_cases_per_million ~ iso_code) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(rep = 1000, type = \"permute\")\n\nnull_dist_sample\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\nNull Hypothesis: independence\n# A tibble: 504,000 × 3\n# Groups:   replicate [1,000]\n   new_cases_per_million iso_code replicate\n                   <dbl> <fct>        <int>\n 1                  16.2 GBR              1\n 2                  11.1 GBR              1\n 3                 595.  GBR              1\n 4                 848.  GBR              1\n 5                 404.  GBR              1\n 6                  77.4 GBR              1\n 7                  79.4 GBR              1\n 8                 716.  GBR              1\n 9                 513.  GBR              1\n10                 232.  GBR              1\n# … with 503,990 more rows\n\nnull_dist_sample <- null_dist_sample %>% \n  calculate(stat = \"diff in medians\", order = c(\"GBR\", \"USA\"))\n\nnull_dist_sample\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate   stat\n       <int>  <dbl>\n 1         1 -5.23 \n 2         2  6.88 \n 3         3 16.3  \n 4         4 11.3  \n 5         5  6.08 \n 6         6 37.1  \n 7         7 -6.19 \n 8         8 -0.401\n 9         9 16.1  \n10        10  5.45 \n# … with 990 more rows\n\nNow we can visualize our generated data. Think of it this way: Through some mathematical tricks, infer tried to simulate what our data would look like in a population where the null hypothesis were true.\n\n\nnull_dist_sample %>% \n  visualise()\n\n\n\n\nWe see a histogram which represents the data of the 1000 reps we generated above. We see that the majority of reps calculated a diff in medians somewhere around zero. This makes sense, since this data is ought to be generated in a population where there is no true difference between new cases (null hypothesis) and all difference there is should be due to random chance.\nNow, let’s see where in that diagram our acutal, observed difference in medians (3.576) would fall:\n\n\nnull_dist_sample %>% \n  visualise() +\n    shade_p_value(observed_diff_in_medians,\n                direction = \"two-sided\")\n\n\n\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  1.22\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.828\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      1.22  498.   0.222 two.sided       25.0    -15.2     65.3\n\nAnother one…\nTo further strengthen our infer and statistical knowledge, let’s look at another country. Maybe Germany had significantly lower cases on average than the USA?\nCharts.\n\n\ndata %>% \n  filter(iso_code %in% c(\"DEU\", \"USA\")) %>% \n  ggplot(aes(x = iso_code, y = new_cases_per_million)) +\n  stat_summary(fun=mean, geom=\"point\", size=5, color=\"red\", fill=\"red\") +\n  geom_jitter(alpha = 0.25) + \n  geom_boxplot(alpha = 0.75) +\n  theme_minimal()\n\n\n\n\nOn the first glance, it’s still a close race, however, there seems to be more of a difference in medians.\nWe further added a red dot to each box which represents the mean value of new_cases_per_million (notice the difference between mean and median). The means seem to be apart even further.\nInfer\nLet’s take more in-depth look with infer. Again, we start off by calculating the difference in means which we observed in our data.\nThen we let infer generate data under the assumption that the reported new cases per million are independent of the country they occurred in (null hypothesis). When using type = permute, infer does this by reassigning the values of new_cases_per_million by chance to either one of the countries. For each rep, our entire data get’s shuffled once. Thus, a possible link between both variables get’s broken.\nSubsequently, we also calculate the difference in means for each of our reps. Because we assumed the null hypothesis to be true, this calculation should result in a normal distribution around 0.\n\n\n# Calculate difference in means in observed data\nobserved_mean_diff_usa_deu <- data %>% \n  filter(iso_code %in% c(\"USA\", \"DEU\")) %>% \n  specify(new_cases_per_million ~ iso_code) %>% \n  calculate(\"diff in means\", order = c(\"USA\", \"DEU\"))\n\nobserved_mean_diff_usa_deu\n\n\nResponse: new_cases_per_million (numeric)\nExplanatory: iso_code (factor)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1  136.\n\n# generate a null distribution based on null hypothesis (independence)\n# then calculate the differences in means in that generated \"population\"\nnull_dist_usa_deu <- data %>% \n  filter(iso_code %in% c(\"USA\", \"DEU\")) %>% \n  specify(new_cases_per_million ~ iso_code) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(\"diff in means\", order = c(\"USA\", \"DEU\"))\n\nnull_dist_usa_deu %>% \n  visualise()\n\n\n\n\nWe see that the difference in means we observed equals 136.45.\nBut we might ask ourselves: How sure can we be of this point estimate? Luckily, infer allows us to calculate confidence intervals:\n\n\nci_diff_usa_deu <- null_dist_usa_deu %>% \n  get_confidence_interval(point_estimate = observed_mean_diff_usa_deu,\n                          type = \"se\", # standard error\n                          level = .95)\n\n# Visualize the diff in means in generated null distribution \n# and the *observed* diff in means\nnull_dist_usa_deu %>% \n  visualise() +\n  shade_p_value(observed_mean_diff_usa_deu,\n                direction = \"two-sided\") +\n  shade_confidence_interval(ci_diff_usa_deu)\n\n\n\n\nAlready we can see that our observed difference in means if much, much higher than anything we calculated in our data which assumed the null hypothesis.\nThus, the possibility of actually observing a difference in means this large (or larger) in a population where the null hypothesis is true, is very small.\nIf we wanted to put an exact number on that possibility we could calculate the p-value.\n\n\np_value_mean_usa_deu <- null_dist_usa_deu %>% \n  get_p_value(obs_stat = observed_mean_diff_usa_deu,\n              direction = \"two-sided\")\n\np_value_mean_usa_deu\n\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\nSo, in a world where the number of new covid cases was independent of the country they appeared in, the possibility of observing a difference in means of cases by country of 136.4525119 (or more) would be 0.\n\n# A tibble: 1 × 7\n  statistic  t_df  p_value alternative estimate lower_ci upper_ci\n      <dbl> <dbl>    <dbl> <chr>          <dbl>    <dbl>    <dbl>\n1      9.01  350. 1.38e-17 two.sided       136.     107.     166.\n\n\n\n\n",
    "preview": "posts/2021-09-12-trying-out-infer/trying-out-infer_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-09-26T03:05:30+02:00",
    "input_file": "trying-out-infer.knit.md",
    "preview_width": 1248,
    "preview_height": 576
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-10T01:34:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-10-das-ist-ein-test-post/",
    "title": "Das ist ein Test-Post",
    "description": "Das ist die Beschreibung des Test Posts",
    "author": [
      {
        "name": "the Ced",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\nDas ist die Überschrift\r\nKorrelation\r\nDie Frage ist: Besteht eine Korrelation zwischen Leistung und Benzin-Verbrauch?\r\nDazu schauen wir uns zunächst einen einfachen Scatterplot an:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-10-das-ist-ein-test-post/das-ist-ein-test-post_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-10T02:22:37+02:00",
    "input_file": "das-ist-ein-test-post.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
