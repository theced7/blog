[
  {
    "path": "posts/2021-09-12-trying-out-infer/",
    "title": "Trying out infer",
    "description": "This is ought to be a short blog post, firstly to get to grips with Distill, secondly to try out infer.",
    "author": [
      {
        "name": "theCed7",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-12",
    "categories": [],
    "contents": "\r\nGetting started\r\nFirst things first: Let’s load infer and the rest of the tidyverse:\r\n\r\n\r\nlibrary(infer)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nInfer is a tidyverse package which aims to aid statistical testing. As such, we also need some data to conduct statistical testing on. As it is 2021, we might as well use something coronavirus-related. This download the vast Our World in Data source file and reads it into R. I will also focus on the UK, since it’s one of the countries with the most cohesive data available.\r\n\r\n\r\n#download.file(\"https://github.com/owid/covid-19-data/raw/master/public/data/owid-covid-data.csv\", \"owid-covid-data.csv\")\r\n\r\ndata <- read_csv(\"owid-covid-data.csv\", \r\n                 col_types = cols(date = col_date(format = \"%Y-%m-%d\")))\r\n\r\ndata <- data %>%\r\n  filter(iso_code %in% c(\"GBR\", \"USA\", \"DEU\")) %>%\r\n  filter(date > \"2021-01-01\")\r\n\r\nglimpse(data)\r\n\r\n\r\nRows: 756\r\nColumns: 62\r\n$ iso_code                              <chr> \"DEU\", \"DEU\", \"DEU\", \"~\r\n$ continent                             <chr> \"Europe\", \"Europe\", \"E~\r\n$ location                              <chr> \"Germany\", \"Germany\", ~\r\n$ date                                  <date> 2021-01-02, 2021-01-0~\r\n$ total_cases                           <dbl> 1773540, 1783896, 1796~\r\n$ new_cases                             <dbl> 10903, 10356, 12320, 1~\r\n$ new_cases_smoothed                    <dbl> 18185.71, 17893.86, 17~\r\n$ total_deaths                          <dbl> 34480, 34791, 35748, 3~\r\n$ new_deaths                            <dbl> 335, 311, 957, 1009, 1~\r\n$ new_deaths_smoothed                   <dbl> 647.714, 642.000, 657.~\r\n$ total_cases_per_million               <dbl> 21138.62, 21262.05, 21~\r\n$ new_cases_per_million                 <dbl> 129.952, 123.432, 146.~\r\n$ new_cases_smoothed_per_million        <dbl> 216.753, 213.275, 210.~\r\n$ total_deaths_per_million              <dbl> 410.963, 414.670, 426.~\r\n$ new_deaths_per_million                <dbl> 3.993, 3.707, 11.406, ~\r\n$ new_deaths_smoothed_per_million       <dbl> 7.720, 7.652, 7.838, 7~\r\n$ reproduction_rate                     <dbl> 0.94, 0.96, 0.98, 1.00~\r\n$ icu_patients                          <dbl> 5703, 5745, 5723, 5650~\r\n$ icu_patients_per_million              <dbl> 67.973, 68.474, 68.212~\r\n$ hosp_patients                         <dbl> NA, NA, NA, NA, NA, NA~\r\n$ hosp_patients_per_million             <dbl> NA, NA, NA, NA, NA, NA~\r\n$ weekly_icu_admissions                 <dbl> NA, NA, NA, NA, NA, NA~\r\n$ weekly_icu_admissions_per_million     <dbl> NA, NA, NA, NA, NA, NA~\r\n$ weekly_hosp_admissions                <dbl> NA, 7777.015, NA, NA, ~\r\n$ weekly_hosp_admissions_per_million    <dbl> NA, 92.693, NA, NA, NA~\r\n$ new_tests                             <dbl> NA, NA, NA, NA, NA, NA~\r\n$ total_tests                           <dbl> NA, 36016829, NA, NA, ~\r\n$ total_tests_per_thousand              <dbl> NA, 429.280, NA, NA, N~\r\n$ new_tests_per_thousand                <dbl> NA, NA, NA, NA, NA, NA~\r\n$ new_tests_smoothed                    <dbl> 125811, 120818, 128689~\r\n$ new_tests_smoothed_per_thousand       <dbl> 1.500, 1.440, 1.534, 1~\r\n$ positive_rate                         <dbl> NA, 0.154, NA, NA, NA,~\r\n$ tests_per_case                        <dbl> NA, 6.5, NA, NA, NA, N~\r\n$ tests_units                           <chr> \"tests performed\", \"te~\r\n$ total_vaccinations                    <dbl> 280781, 303333, 349385~\r\n$ people_vaccinated                     <dbl> 279853, 302401, 348402~\r\n$ people_fully_vaccinated               <dbl> 928, 932, 983, 1070, 1~\r\n$ total_boosters                        <dbl> NA, NA, NA, NA, NA, NA~\r\n$ new_vaccinations                      <dbl> 48088, 22552, 46052, 5~\r\n$ new_vaccinations_smoothed             <dbl> 42738, 39854, 43848, 4~\r\n$ total_vaccinations_per_hundred        <dbl> 0.33, 0.36, 0.42, 0.48~\r\n$ people_vaccinated_per_hundred         <dbl> 0.33, 0.36, 0.42, 0.48~\r\n$ people_fully_vaccinated_per_hundred   <dbl> 0.00, 0.00, 0.00, 0.00~\r\n$ total_boosters_per_hundred            <dbl> NA, NA, NA, NA, NA, NA~\r\n$ new_vaccinations_smoothed_per_million <dbl> 509, 475, 523, 526, 52~\r\n$ stringency_index                      <dbl> 82.41, 82.41, 82.41, 8~\r\n$ population                            <dbl> 83900471, 83900471, 83~\r\n$ population_density                    <dbl> 237.016, 237.016, 237.~\r\n$ median_age                            <dbl> 46.6, 46.6, 46.6, 46.6~\r\n$ aged_65_older                         <dbl> 21.453, 21.453, 21.453~\r\n$ aged_70_older                         <dbl> 15.957, 15.957, 15.957~\r\n$ gdp_per_capita                        <dbl> 45229.25, 45229.25, 45~\r\n$ extreme_poverty                       <dbl> NA, NA, NA, NA, NA, NA~\r\n$ cardiovasc_death_rate                 <dbl> 156.139, 156.139, 156.~\r\n$ diabetes_prevalence                   <dbl> 8.31, 8.31, 8.31, 8.31~\r\n$ female_smokers                        <dbl> 28.2, 28.2, 28.2, 28.2~\r\n$ male_smokers                          <dbl> 33.1, 33.1, 33.1, 33.1~\r\n$ handwashing_facilities                <dbl> NA, NA, NA, NA, NA, NA~\r\n$ hospital_beds_per_thousand            <dbl> 8, 8, 8, 8, 8, 8, 8, 8~\r\n$ life_expectancy                       <dbl> 81.33, 81.33, 81.33, 8~\r\n$ human_development_index               <dbl> 0.947, 0.947, 0.947, 0~\r\n$ excess_mortality                      <dbl> NA, 38.85, NA, NA, NA,~\r\n\r\nStarting off with infer\r\nThe idea of infer is to simplfy statistical testing into four easy steps:\r\nspecify()\r\nhypothesize()\r\ngenerate()\r\ncalculate()\r\nWe now go through each of these steps one by one.\r\nspecify: What do we want to know?\r\nFirst of all, we have to clarify what issue we want to test. What variable of our data frame are we interested in? In this blog post we will ask ourselves, whether there was a significant difference between the reported incidence of SARS-Cov-2 cases between Great Britain and the USA. Has one country had on average less new cases (per million) than the other?`\r\nSo we filter the data to compare Great Britain and the USA. Then we specify what variables we are interested in. We also declare, what we believe is cause and effect, e. g. the different countries should explain (explanatory), why there is a difference in new cases (response).\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  specify(explanatory = iso_code, response = new_cases_per_million )\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\n# A tibble: 504 x 2\r\n   new_cases_per_million iso_code\r\n                   <dbl> <fct>   \r\n 1                  848. GBR     \r\n 2                  809. GBR     \r\n 3                  864. GBR     \r\n 4                  896. GBR     \r\n 5                  917. GBR     \r\n 6                  774. GBR     \r\n 7                 1000. GBR     \r\n 8                  881. GBR     \r\n 9                  807. GBR     \r\n10                  678. GBR     \r\n# ... with 494 more rows\r\n\r\nhypothesize: declaring the null hypothesis\r\nSecondly, we declare the null hypothesis. Statistically speaking, the null hypothesis assumes there truly is no difference (or no correlation). Applied to our example our null hypothesis would be: There is no significant difference in new SARS-Cov2 cases per million people between the US and the UK.\r\nOn a side note: We also introduce a new, shorter way of using specify. Keep an eye on which order you place the arguments (i. e. column names) in, because that determines which is used as the dependent or independent variable.\r\nDo not pay a lot of attention on the output our code is generating at this moment. Hopefully, this all comes together shortly. As long as there aren’t any errors and you roughly understand the statistical lingo, we should be fine.\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  specify(new_cases_per_million ~ iso_code) %>% \r\n  hypothesize(null = \"independence\")\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\nNull Hypothesis: independence\r\n# A tibble: 504 x 2\r\n   new_cases_per_million iso_code\r\n                   <dbl> <fct>   \r\n 1                  848. GBR     \r\n 2                  809. GBR     \r\n 3                  864. GBR     \r\n 4                  896. GBR     \r\n 5                  917. GBR     \r\n 6                  774. GBR     \r\n 7                 1000. GBR     \r\n 8                  881. GBR     \r\n 9                  807. GBR     \r\n10                  678. GBR     \r\n# ... with 494 more rows\r\n\r\ngenerate: generating the null distribution\r\nNot quite sure what happens here.\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  specify(iso_code ~ new_cases_per_million) %>% \r\n  hypothesize(null = \"independence\") %>% \r\n  generate(reps = 1000, type = \"permute\")\r\n\r\n\r\nResponse: iso_code (factor)\r\nExplanatory: new_cases_per_million (numeric)\r\nNull Hypothesis: independence\r\n# A tibble: 504,000 x 3\r\n# Groups:   replicate [1,000]\r\n   iso_code new_cases_per_million replicate\r\n   <fct>                    <dbl>     <int>\r\n 1 GBR                       848.         1\r\n 2 USA                       809.         1\r\n 3 USA                       864.         1\r\n 4 USA                       896.         1\r\n 5 USA                       917.         1\r\n 6 USA                       774.         1\r\n 7 GBR                      1000.         1\r\n 8 USA                       881.         1\r\n 9 USA                       807.         1\r\n10 USA                       678.         1\r\n# ... with 503,990 more rows\r\n\r\ncalculate: calculating summary statistics\r\nNow that we have some understanding of how to use the infer functions, we try to get back to our original question.\r\nShort interlude: Visualization\r\nBefore we move on with facts, figures and infer, let’s try to get a rough picture of our question with some basic data visualization.\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  ggplot(aes(x = iso_code, y = new_cases_per_million)) +\r\n  geom_jitter(alpha = 0.25) + \r\n  geom_boxplot(alpha = 0.75) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nHere we already see that both medians lie quite close together. So, aren’t these countries so different after all?\r\nBack to infer: t-Test\r\nSo, let’s calculate how big the difference between both medians actually is:\r\n\r\n\r\nobserved_diff_in_medians <- data %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  specify(new_cases_per_million ~ iso_code) %>% \r\n  calculate(stat = \"diff in medians\", order = c(\"USA\", \"GBR\"))\r\n\r\nobserved_diff_in_medians\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\n# A tibble: 1 x 1\r\n   stat\r\n  <dbl>\r\n1  3.58\r\n\r\nSo, stat = diff in medians does what it says: It calculates the difference in medians. The order specifies the order of the subtraction (i. e. changes the sign.)\r\nJust for illustration purposes, we can also try to calculate the difference in medians without infer:\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  group_by(iso_code) %>% \r\n  summarise(\r\n    median = median(new_cases_per_million),\r\n  ) %>% \r\n  summarise(\r\n    iso_code = iso_code,\r\n    median = median,\r\n    diff = diff(median),\r\n  )\r\n\r\n\r\n# A tibble: 2 x 3\r\n  iso_code median  diff\r\n  <chr>     <dbl> <dbl>\r\n1 GBR        173.  3.58\r\n2 USA        177.  3.58\r\n\r\nBut back to infer: We know now the difference between medians of new_cases_per_million in both countries equals 3.576. Is this enough do discard our null hypothesis?\r\nLet’s use infer to generate some data under the assumption, that in fact, the null hypothesis is true and there genuinely is no significant difference in new cases between both countries. Then we calculate the diff in medians for each replicate of the generated data.\r\n\r\n\r\nnull_dist_sample <- data %>% \r\n  filter(iso_code %in% c(\"GBR\", \"USA\")) %>% \r\n  specify(new_cases_per_million ~ iso_code) %>% \r\n  hypothesise(null = \"independence\") %>% \r\n  generate(rep = 1000, type = \"permute\")\r\n\r\nnull_dist_sample\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\nNull Hypothesis: independence\r\n# A tibble: 504,000 x 3\r\n# Groups:   replicate [1,000]\r\n   new_cases_per_million iso_code replicate\r\n                   <dbl> <fct>        <int>\r\n 1                  39.5 GBR              1\r\n 2                 232.  GBR              1\r\n 3                  64.4 GBR              1\r\n 4                 156.  GBR              1\r\n 5                 607.  GBR              1\r\n 6                 188.  GBR              1\r\n 7                 424.  GBR              1\r\n 8                  28.2 GBR              1\r\n 9                 344.  GBR              1\r\n10                 467.  GBR              1\r\n# ... with 503,990 more rows\r\n\r\nnull_dist_sample <- null_dist_sample %>% \r\n  calculate(stat = \"diff in medians\", order = c(\"GBR\", \"USA\"))\r\n\r\nnull_dist_sample\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\nNull Hypothesis: independence\r\n# A tibble: 1,000 x 2\r\n   replicate   stat\r\n       <int>  <dbl>\r\n 1         1 -21.0 \r\n 2         2  32.3 \r\n 3         3  15.1 \r\n 4         4  10.7 \r\n 5         5 -10.0 \r\n 6         6  -8.54\r\n 7         7 -12.6 \r\n 8         8  21.8 \r\n 9         9   9.69\r\n10        10 -19.9 \r\n# ... with 990 more rows\r\n\r\nNow we can visualize our generated data. Think of it this way: Through some mathematical tricks, infer tried to simulate what our data would look like in a population where the null hypothesis were true.\r\n\r\n\r\nnull_dist_sample %>% \r\n  visualise()\r\n\r\n\r\n\r\n\r\nWe see a histogram which represents the data of the 1000 reps we generated above. We see that the majority of reps calculated a diff in medians somewhere around zero. This makes sense, since this data is ought to be generated in a population where there is no true difference between new cases (null hypothesis) and all difference there is should be due to random chance.\r\nNow, let’s see where in that diagram our acutal, observed difference in medians (3.576) would fall:\r\n\r\n\r\nnull_dist_sample %>% \r\n  visualise() +\r\n    shade_p_value(observed_diff_in_medians,\r\n                direction = \"two-sided\")\r\n\r\n\r\n\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\n# A tibble: 1 x 1\r\n   stat\r\n  <dbl>\r\n1  1.22\r\n\r\n# A tibble: 1 x 1\r\n  p_value\r\n    <dbl>\r\n1     0.9\r\n# A tibble: 1 x 7\r\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\r\n      <dbl> <dbl>   <dbl> <chr>          <dbl>    <dbl>    <dbl>\r\n1      1.22  498.   0.222 two.sided       25.0    -15.2     65.3\r\n\r\nAnother one…\r\nTo further strengthen our infer and statistical knowledge, let’s look at another country. Maybe Germany had significantly lower cases on average than the USA?\r\nCharts.\r\n\r\n\r\ndata %>% \r\n  filter(iso_code %in% c(\"DEU\", \"USA\")) %>% \r\n  ggplot(aes(x = iso_code, y = new_cases_per_million)) +\r\n  stat_summary(fun=mean, geom=\"point\", size=5, color=\"red\", fill=\"red\") +\r\n  geom_jitter(alpha = 0.25) + \r\n  geom_boxplot(alpha = 0.75) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nOn the first glance, it’s still a close race, however, there seems to be more of a difference in medians.\r\nWe further added a red dot to each box which represents the mean value of new_cases_per_million (notice the difference between mean and median). The means seem to be apart even further.\r\nInfer\r\nLet’s take more in-depth look with infer. Again, we start off by calculating the difference in means which we observed in our data.\r\nThen we let infer generate data which simulates a population where the null hypothesis is true. In this generated data where we also calculate the difference between means.\r\n\r\n\r\n# Calculate difference in means in observed data\r\nobserved_mean_diff_usa_deu <- data %>% \r\n  filter(iso_code %in% c(\"USA\", \"DEU\")) %>% \r\n  specify(new_cases_per_million ~ iso_code) %>% \r\n  calculate(\"diff in means\", order = c(\"USA\", \"DEU\"))\r\n\r\nobserved_mean_diff_usa_deu\r\n\r\n\r\nResponse: new_cases_per_million (numeric)\r\nExplanatory: iso_code (factor)\r\n# A tibble: 1 x 1\r\n   stat\r\n  <dbl>\r\n1  136.\r\n\r\n# generate a null distribution based on null hypothesis (independence)\r\n# then calculate the differences in means in that generated \"population\"\r\nnull_dist_usa_deu <- data %>% \r\n  filter(iso_code %in% c(\"USA\", \"DEU\")) %>% \r\n  specify(new_cases_per_million ~ iso_code) %>% \r\n  hypothesise(null = \"independence\") %>% \r\n  generate(reps = 1000, type = \"permute\") %>% \r\n  calculate(\"diff in means\", order = c(\"USA\", \"DEU\"))\r\n\r\nnull_dist_usa_deu %>% \r\n  visualise()\r\n\r\n\r\n\r\n\r\nWe see that the difference in means between both countries equals 136.45.\r\nAnother very interesting feature of infer is it’s ability to calculate confidence intervals.\r\n\r\n\r\n# Calculate confidence intervals for our observed value in generated null dist.\r\nci_diff_usa_deu <- null_dist_usa_deu %>% \r\n  get_confidence_interval(point_estimate = observed_mean_diff_usa_deu,\r\n                          type = \"se\",\r\n                          level = .95)\r\n\r\n# Visualize the diff in means in generated null distribution \r\n# and the *observed* diff in means\r\nnull_dist_usa_deu %>% \r\n  visualise() +\r\n  shade_p_value(observed_mean_diff_usa_deu,\r\n                direction = \"two-sided\") +\r\n  shade_confidence_interval(ci_diff_usa_deu)\r\n\r\n\r\n\r\np_value_median_usa_deu <- null_dist_usa_deu %>% \r\n  get_p_value(obs_stat = observed_mean_diff_usa_deu,\r\n              direction = \"two-sided\")\r\n\r\np_value_median_usa_deu\r\n\r\n\r\n# A tibble: 1 x 1\r\n  p_value\r\n    <dbl>\r\n1       0\r\n\r\ndata %>% \r\n  t_test(formula = new_cases_per_million ~ iso_code,\r\n         direction = \"two-sided\",\r\n         order = c(\"USA\", \"DEU\"))\r\n\r\n\r\n# A tibble: 1 x 7\r\n  statistic  t_df  p_value alternative estimate lower_ci upper_ci\r\n      <dbl> <dbl>    <dbl> <chr>          <dbl>    <dbl>    <dbl>\r\n1      9.01  350. 1.38e-17 two.sided       136.     107.     166.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-12-trying-out-infer/trying-out-infer_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-09-23T03:00:55+02:00",
    "input_file": "trying-out-infer.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-10T01:34:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-10-das-ist-ein-test-post/",
    "title": "Das ist ein Test-Post",
    "description": "Das ist die Beschreibung des Test Posts",
    "author": [
      {
        "name": "the Ced",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\nDas ist die Überschrift\r\nKorrelation\r\nDie Frage ist: Besteht eine Korrelation zwischen Leistung und Benzin-Verbrauch?\r\nDazu schauen wir uns zunächst einen einfachen Scatterplot an:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-10-das-ist-ein-test-post/das-ist-ein-test-post_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-10T02:22:37+02:00",
    "input_file": "das-ist-ein-test-post.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
