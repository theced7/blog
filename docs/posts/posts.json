[
  {
    "path": "posts/2021-09-12-trying-out-infer/",
    "title": "Trying out infer",
    "description": "This is ought to be a short blog post, firstly to get to grips with Distill, secondly to try out infer.",
    "author": [
      {
        "name": "theCed7",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-12",
    "categories": [],
    "contents": "\nGetting started\nFirst things first: Let’s load infer and the rest of the tidyverse:\n\n\nlibrary(infer)\nlibrary(tidyverse)\n\n\n\nInfer is a tidyverse package which aims to aid statistical testing. As such, we also need some data to conduct statistical testing on. As it is 2021, we might as well use something coronavirus-related. This download the vast Our World in Data source file and reads it into R. I will also focus on the UK, since it’s one of the countries with the most cohesive data available.\n\n\n#download.file(\"https://github.com/owid/covid-19-data/raw/master/public/data/owid-covid-data.csv\", \"owid-covid-data.csv\")\n\ndata <- read_csv(\"owid-covid-data.csv\", \n                 col_types = cols(date = col_date(format = \"%Y-%m-%d\")))\n\ndata <- data %>%\n  filter(iso_code == \"GBR\") %>%\n  filter(date > \"2021-01-01\")\n\nglimpse(data)\n\n\nRows: 252\nColumns: 62\n$ iso_code                              <chr> \"GBR\", \"GBR\", \"GBR\", \"…\n$ continent                             <chr> \"Europe\", \"Europe\", \"E…\n$ location                              <chr> \"United Kingdom\", \"Uni…\n$ date                                  <date> 2021-01-02, 2021-01-0…\n$ total_cases                           <dbl> 2607546, 2662703, 2721…\n$ new_cases                             <dbl> 57853, 55157, 58923, 6…\n$ new_cases_smoothed                    <dbl> 49258.14, 52495.86, 54…\n$ total_deaths                          <dbl> 74682, 75137, 75547, 7…\n$ new_deaths                            <dbl> 445, 455, 410, 881, 10…\n$ new_deaths_smoothed                   <dbl> 595.571, 611.000, 618.…\n$ total_cases_per_million               <dbl> 38229.82, 39038.49, 39…\n$ new_cases_per_million                 <dbl> 848.196, 808.669, 863.…\n$ new_cases_smoothed_per_million        <dbl> 722.185, 769.654, 806.…\n$ total_deaths_per_million              <dbl> 1094.930, 1101.601, 11…\n$ new_deaths_per_million                <dbl> 6.524, 6.671, 6.011, 1…\n$ new_deaths_smoothed_per_million       <dbl> 8.732, 8.958, 9.069, 9…\n$ reproduction_rate                     <dbl> 1.33, 1.31, 1.29, 1.25…\n$ icu_patients                          <dbl> 2256, 2420, 2560, 2645…\n$ icu_patients_per_million              <dbl> 33.076, 35.480, 37.533…\n$ hosp_patients                         <dbl> 27561, 29033, 30775, 3…\n$ hosp_patients_per_million             <dbl> 404.078, 425.659, 451.…\n$ weekly_icu_admissions                 <dbl> NA, NA, NA, NA, NA, NA…\n$ weekly_icu_admissions_per_million     <dbl> NA, NA, NA, NA, NA, NA…\n$ weekly_hosp_admissions                <dbl> NA, 23065, NA, NA, NA,…\n$ weekly_hosp_admissions_per_million    <dbl> NA, 338.161, NA, NA, N…\n$ new_tests                             <dbl> 410261, 416962, 461900…\n$ total_tests                           <dbl> 45830851, 46247813, 46…\n$ total_tests_per_thousand              <dbl> 671.937, 678.050, 684.…\n$ new_tests_per_thousand                <dbl> 6.015, 6.113, 6.772, 7…\n$ new_tests_smoothed                    <dbl> 389356, 398536, 414358…\n$ new_tests_smoothed_per_thousand       <dbl> 5.708, 5.843, 6.075, 6…\n$ positive_rate                         <dbl> 0.127, 0.132, 0.133, 0…\n$ tests_per_case                        <dbl> 7.9, 7.6, 7.5, 7.8, 7.…\n$ tests_units                           <chr> \"tests performed\", \"te…\n$ total_vaccinations                    <dbl> NA, 1380430, NA, NA, N…\n$ people_vaccinated                     <dbl> NA, 1380430, NA, NA, N…\n$ people_fully_vaccinated               <dbl> NA, NA, NA, NA, NA, NA…\n$ total_boosters                        <dbl> NA, NA, NA, NA, NA, NA…\n$ new_vaccinations                      <dbl> NA, NA, NA, NA, NA, NA…\n$ new_vaccinations_smoothed             <dbl> 52692, 53622, 72443, 9…\n$ total_vaccinations_per_hundred        <dbl> NA, 2.02, NA, NA, NA, …\n$ people_vaccinated_per_hundred         <dbl> NA, 2.02, NA, NA, NA, …\n$ people_fully_vaccinated_per_hundred   <dbl> NA, NA, NA, NA, NA, NA…\n$ total_boosters_per_hundred            <dbl> NA, NA, NA, NA, NA, NA…\n$ new_vaccinations_smoothed_per_million <dbl> 773, 786, 1062, 1338, …\n$ stringency_index                      <dbl> 79.63, 79.63, 79.63, 8…\n$ population                            <dbl> 68207114, 68207114, 68…\n$ population_density                    <dbl> 272.898, 272.898, 272.…\n$ median_age                            <dbl> 40.8, 40.8, 40.8, 40.8…\n$ aged_65_older                         <dbl> 18.517, 18.517, 18.517…\n$ aged_70_older                         <dbl> 12.527, 12.527, 12.527…\n$ gdp_per_capita                        <dbl> 39753.24, 39753.24, 39…\n$ extreme_poverty                       <dbl> 0.2, 0.2, 0.2, 0.2, 0.…\n$ cardiovasc_death_rate                 <dbl> 122.137, 122.137, 122.…\n$ diabetes_prevalence                   <dbl> 4.28, 4.28, 4.28, 4.28…\n$ female_smokers                        <dbl> 20, 20, 20, 20, 20, 20…\n$ male_smokers                          <dbl> 24.7, 24.7, 24.7, 24.7…\n$ handwashing_facilities                <dbl> NA, NA, NA, NA, NA, NA…\n$ hospital_beds_per_thousand            <dbl> 2.54, 2.54, 2.54, 2.54…\n$ life_expectancy                       <dbl> 81.32, 81.32, 81.32, 8…\n$ human_development_index               <dbl> 0.932, 0.932, 0.932, 0…\n$ excess_mortality                      <dbl> NA, 25.47, NA, NA, NA,…\n\nStarting off with infer\nThe idea of infer is to simplfy statistical testing into four easy steps:\nspecify()\nhypothesize()\ngenerate()\ncalculate()\nWe now go through each of these steps one by one.\nspecify: What do we want to know?\nFirst of all, we have to clarify what issue we want to test. What variable of our data frame are we interested in? Here, we will ask: Is there a correlation between new cases and the current reproduction rate?\n\n\ndata %>% \n  specify(new_cases ~ reproduction_rate)\n\n\nResponse: new_cases (numeric)\nExplanatory: reproduction_rate (numeric)\n# A tibble: 250 × 2\n   new_cases reproduction_rate\n       <dbl>             <dbl>\n 1     57853              1.33\n 2     55157              1.31\n 3     58923              1.29\n 4     61087              1.25\n 5     62556              1.19\n 6     52787              1.13\n 7     68192              1.13\n 8     60098              1.09\n 9     55026              1.04\n10     46275              0.98\n# … with 240 more rows\n\nhypothesize: declaring the null hypothesis\nSecondly, we declare the null hypothesis. Statistically speaking, the null hypothesis assumes there is truly no difference (or no correlation).\n\nResponse: new_cases (numeric)\nExplanatory: reproduction_rate (numeric)\nNull Hypothesis: independence\n# A tibble: 250 × 2\n   new_cases reproduction_rate\n       <dbl>             <dbl>\n 1     57853              1.33\n 2     55157              1.31\n 3     58923              1.29\n 4     61087              1.25\n 5     62556              1.19\n 6     52787              1.13\n 7     68192              1.13\n 8     60098              1.09\n 9     55026              1.04\n10     46275              0.98\n# … with 240 more rows\n\ngenerate: generating the null distribution\n\nResponse: new_cases (numeric)\nExplanatory: reproduction_rate (numeric)\nNull Hypothesis: independence\n# A tibble: 250,000 × 3\n# Groups:   replicate [1,000]\n   new_cases reproduction_rate replicate\n       <dbl>             <dbl>     <int>\n 1      9947              1.33         1\n 2      5597              1.31         1\n 3     28344              1.29         1\n 4      1984              1.25         1\n 5      3124              1.19         1\n 6      2490              1.13         1\n 7      2459              1.13         1\n 8     47662              1.09         1\n 9      7322              1.04         1\n10     68192              0.98         1\n# … with 249,990 more rows\n\ncalculate: calculating summary statistics\n\nResponse: new_cases (numeric)\nExplanatory: reproduction_rate (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       <int>   <dbl>\n 1         1  0.0720\n 2         2  0.0390\n 3         3  0.103 \n 4         4 -0.0568\n 5         5 -0.0258\n 6         6 -0.0183\n 7         7  0.0934\n 8         8 -0.0445\n 9         9  0.0432\n10        10  0.0662\n# … with 990 more rows\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-12T02:09:51+02:00",
    "input_file": "trying-out-infer.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-10T01:34:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-10-das-ist-ein-test-post/",
    "title": "Das ist ein Test-Post",
    "description": "Das ist die Beschreibung des Test Posts",
    "author": [
      {
        "name": "the Ced",
        "url": "https://github.com/theced7"
      }
    ],
    "date": "2021-09-10",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\nDas ist die Überschrift\r\nKorrelation\r\nDie Frage ist: Besteht eine Korrelation zwischen Leistung und Benzin-Verbrauch?\r\nDazu schauen wir uns zunächst einen einfachen Scatterplot an:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-10-das-ist-ein-test-post/das-ist-ein-test-post_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-10T02:22:37+02:00",
    "input_file": "das-ist-ein-test-post.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
